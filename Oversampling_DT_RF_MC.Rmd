---
title: "Untitled"
author: "Fabian Garrett"
date: "2024-04-11"
output: html_document
---

```{r}
freq_table <- table(data_customer_segmentation$target_categorical)
cbind(Frequency = freq_table,
      Proportion = round(prop.table(freq_table), 3))
```
```{r}
library(ROSE)

# Calcular o número de instâncias na classe majoritária
num_majoritaria <- sum(data_customer_segmentation$target_categorical == "non.buyer")

# Realizar oversampling
oversampled_data <- ovun.sample(target_categorical ~ ., 
                                data = data_customer_segmentation, 
                                method = "over", 
                                N = 10^5,  # Um valor grande suficiente
                                seed = 123)$data

# Verificar a distribuição da classe após oversampling
table(oversampled_data$target_categorical)

```
#looking data
```{r}
str(oversampled_data)
```

# Classification class distribution in full dataset
```{r}
freq_table <- table(oversampled_data$target_categorical)
cbind(Frequency = freq_table,
      Proportion = round(prop.table(freq_table), 3))
```
# Define train and test sets ----------------------------------------------
```{r}
set.seed(123) 
trainIndex <- createDataPartition(oversampled_data$target_categorical, p = .8, 
                                  list = FALSE, 
                                  times = 1)
data_train <- oversampled_data[trainIndex, ]
data_test <- oversampled_data[-trainIndex, ]

```


# metric roc
```{r}
metric <- "ROC"
control <- trainControl(method = "repeatedcv",
                        number = 5, # number of folds
                        repeats = 3, # number of repetitions
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        savePredictions = TRUE)
```
# Learn Decision Tree (rpart)
```{r}
seed <- 123
set.seed(seed)
model_DT <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster ,
                  data = oversampled_data,
                  method = "rpart", # Decision Trees, with CART algorithm
                  metric = metric,
                  trControl = control)
```
# Learn Random Forest (rf)
```{r}
set.seed(seed)
model_RF <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster ,
                  data = oversampled_data,
                  method = "rf",  # Random Forests
                  metric = metric,
                  trControl = control)
```
#' Check models, metrics and specific tunning parameters
#'   cp: Complexity parameter
#'   mtry: Number of randomly drawn candidate variables
```{r}
print(model_DT)
```
```{r}
print(model_RF)
```
#' Calculate and compare models' performance
#' Note:
#'   - We'll use function 'resamples()' for visualization
#'     of the resampling results
#'   - These calculations are based on resampling the TRAIN set

```{r}
fit_models <- list(Decision_tree = model_DT, 
                   Random_forest = model_RF)
results <- resamples(fit_models) 
dotplot(results)
summary(results)
```
# Check Complexity Parameter (cp) used for trained model
```{r}
print(model_DT$results$cp)
```
# Plot ROC vs Complexity Parameter for model
```{r}
plot(model_DT, main = "Decision Tree")
```
# Check mtry parameter used for trained model
# getModelInfo(model_RF)$rf$parameters
```{r}
print(model_RF$results$mtry)
```
#Plot ROC vs used parameters for model
```{r}
plot(model_RF, main = "Random Forest")
```

# Improve the Random Forest model -----------------------------------------
# To improve the RF model, we fine tune the mtry parameter
```{r}
tune_grid <- expand.grid(mtry = 1:10)

set.seed(seed)
model_RF_tuned <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster ,
                       data = data_train,
                       method = "rf",
                       metric = metric,
                       trControl = control,
                       tuneGrid = tune_grid)
```
# Inspect model
```{r}
print(model_RF_tuned)
```
# Plot ROC vs used parameters
```{r}
plot(model_RF_tuned,
     main = "Random Forest - tuned for mtry parameter")
```
# Compare all models ------------------------------------------------------
# Calculate and compare models' performance
```{r}
it_models <- list(Decision_tree = model_DT, 
                   Random_forest = model_RF,
                   Random_forest_tuned = model_RF_tuned)
results <- resamples(fit_models)
dotplot(results)
summary(results)
```
# Model Validation --------------------------------------------------------
#' To validate the model, we'll make predictions from the TEST set
# Estimate performance of Random Forest on the TEST dataset
```{r}
predictions_prob <- predict(model_RF_tuned, data_test, type = "prob")
head(predictions_prob)
predictions_raw <- predict(model_RF_tuned, data_test, type = "raw")
head(cbind(predictions_prob, predictions_raw))
```



```{r}
# Estatísticas resumidas das probabilidades de previsão para cada classe
summary(predictions_prob$X1)
summary(predictions_prob$X0)

```

# Confusion Matrix
```{r}
confusionMatrix(data = predictions_raw,
                reference = data_test$target_categorical)
                # Tip: use the following argument for more metrics
                #   mode = "everything"
                # Note that:
                #   Precison = Pos Pred Value
                #   Recall = Sensitivity
```





